{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xwlf_rhLM_1_",
        "outputId": "5b796f7a-d68b-40a7-d85f-90d9117f25e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: munkres in /usr/local/lib/python3.11/dist-packages (1.1.4)\n"
          ]
        }
      ],
      "source": [
        "pip install munkres"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HlpQ7eGfQqV2",
        "outputId": "8b2a2ecc-4c45-4b59-fc12-f2f2cb0d6aee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scanpy in /usr/local/lib/python3.11/dist-packages (1.11.2)\n",
            "Requirement already satisfied: anndata>=0.8 in /usr/local/lib/python3.11/dist-packages (from scanpy) (0.11.4)\n",
            "Requirement already satisfied: h5py>=3.7.0 in /usr/local/lib/python3.11/dist-packages (from scanpy) (3.13.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from scanpy) (1.5.1)\n",
            "Requirement already satisfied: legacy-api-wrap>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from scanpy) (1.4.1)\n",
            "Requirement already satisfied: matplotlib>=3.7.5 in /usr/local/lib/python3.11/dist-packages (from scanpy) (3.10.0)\n",
            "Requirement already satisfied: natsort in /usr/local/lib/python3.11/dist-packages (from scanpy) (8.4.0)\n",
            "Requirement already satisfied: networkx>=2.7.1 in /usr/local/lib/python3.11/dist-packages (from scanpy) (3.5)\n",
            "Requirement already satisfied: numba>=0.57.1 in /usr/local/lib/python3.11/dist-packages (from scanpy) (0.60.0)\n",
            "Requirement already satisfied: numpy>=1.24.1 in /usr/local/lib/python3.11/dist-packages (from scanpy) (2.0.2)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.11/dist-packages (from scanpy) (24.2)\n",
            "Requirement already satisfied: pandas>=1.5.3 in /usr/local/lib/python3.11/dist-packages (from scanpy) (2.2.2)\n",
            "Requirement already satisfied: patsy!=1.0.0 in /usr/local/lib/python3.11/dist-packages (from scanpy) (1.0.1)\n",
            "Requirement already satisfied: pynndescent>=0.5.13 in /usr/local/lib/python3.11/dist-packages (from scanpy) (0.5.13)\n",
            "Requirement already satisfied: scikit-learn>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from scanpy) (1.6.1)\n",
            "Requirement already satisfied: scipy>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from scanpy) (1.15.3)\n",
            "Requirement already satisfied: seaborn>=0.13.2 in /usr/local/lib/python3.11/dist-packages (from scanpy) (0.13.2)\n",
            "Requirement already satisfied: session-info2 in /usr/local/lib/python3.11/dist-packages (from scanpy) (0.1.2)\n",
            "Requirement already satisfied: statsmodels>=0.14.4 in /usr/local/lib/python3.11/dist-packages (from scanpy) (0.14.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from scanpy) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from scanpy) (4.14.0)\n",
            "Requirement already satisfied: umap-learn>=0.5.6 in /usr/local/lib/python3.11/dist-packages (from scanpy) (0.5.7)\n",
            "Requirement already satisfied: array-api-compat!=1.5,>1.4 in /usr/local/lib/python3.11/dist-packages (from anndata>=0.8->scanpy) (1.12.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.5->scanpy) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.5->scanpy) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.5->scanpy) (4.58.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.5->scanpy) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.5->scanpy) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.5->scanpy) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.5->scanpy) (2.9.0.post0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.57.1->scanpy) (0.43.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.5.3->scanpy) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.5.3->scanpy) (2025.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.1.3->scanpy) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.7.5->scanpy) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "pip install scanpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oPc1FQYqM57p",
        "outputId": "9a391e7a-464d-4577-dbc1-1e4d8d98efd4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training with configuration:\n",
            "{'num_workers': 4, 'paths': {'data': '.', 'results': './results'}, 'batch_size': 256, 'data_dim': 1000, 'n_classes': 7, 'epochs': 100, 'dataset': 'Pollen', 'learning_rate': 0.001, 'latent_dim': 32, 'save_path': './results/Pollen'}\n",
            "(301, 21721) (301, 21721) keeping 1000 genes\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> Number of classes changed from 11 to 11 <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "(301, 21721) (301, 21721) keeping 1000 genes\n",
            "\tEvalute: [nmi: 0.920688] [ari: 0.933688] [acc: 0.913621]\n",
            "Training completed and results saved.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import random\n",
        "import h5py\n",
        "import scanpy as sc\n",
        "import scipy as sp\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn.functional import binary_cross_entropy_with_logits as bce_logits\n",
        "from torch.nn.functional import mse_loss as mse\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score, normalized_mutual_info_score, adjusted_rand_score, accuracy_score\n",
        "from scipy.optimize import linear_sum_assignment as hungarian\n",
        "from sklearn import metrics\n",
        "from munkres import Munkres\n",
        "\n",
        "# Set CUDA device\n",
        "device = torch.device(\"cpu\")\n",
        "\n",
        "\n",
        "# Utility Classes\n",
        "class AverageMeter(object):\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val, self.avg, self.sum, self.count = 0, 0, 0, 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "# Model Classes\n",
        "class AutoEncoder(torch.nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_genes,\n",
        "        hidden_size=128,\n",
        "        dropout=0,\n",
        "        masked_data_weight=.75,\n",
        "        mask_loss_weight=0.7,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.num_genes = num_genes\n",
        "        self.masked_data_weight = masked_data_weight\n",
        "        self.mask_loss_weight = mask_loss_weight\n",
        "\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Dropout(p=dropout),\n",
        "            nn.Linear(self.num_genes, 256),\n",
        "            nn.LayerNorm(256),\n",
        "            nn.Mish(inplace=True),\n",
        "            nn.Linear(256, hidden_size),\n",
        "            nn.LayerNorm(hidden_size),\n",
        "            nn.Mish(inplace=True),\n",
        "            nn.Linear(hidden_size, hidden_size)\n",
        "        )\n",
        "\n",
        "        self.mask_predictor = nn.Linear(hidden_size, num_genes)\n",
        "        self.decoder = nn.Linear(\n",
        "            in_features=hidden_size+num_genes, out_features=num_genes)\n",
        "\n",
        "    def forward_mask(self, x):\n",
        "        latent = self.encoder(x)\n",
        "        predicted_mask = self.mask_predictor(latent)\n",
        "        reconstruction = self.decoder(\n",
        "            torch.cat([latent, predicted_mask], dim=1))\n",
        "        return latent, predicted_mask, reconstruction\n",
        "\n",
        "    def loss_mask(self, x, y, mask):\n",
        "        latent, predicted_mask, reconstruction = self.forward_mask(x)\n",
        "        w_nums = mask * self.masked_data_weight + (1 - mask) * (1 - self.masked_data_weight)\n",
        "        reconstruction_loss = (1-self.mask_loss_weight) * torch.mul(\n",
        "            w_nums, mse(reconstruction, y, reduction='none'))\n",
        "        mask_loss = self.mask_loss_weight * \\\n",
        "            bce_logits(predicted_mask, mask, reduction=\"mean\")\n",
        "        reconstruction_loss = reconstruction_loss.mean()\n",
        "        loss = reconstruction_loss + mask_loss\n",
        "        return latent, loss\n",
        "\n",
        "    def feature(self, x):\n",
        "        latent = self.encoder(x)\n",
        "        return latent\n",
        "\n",
        "# Data Processing Classes\n",
        "default_svd_params = {\n",
        "    \"n_components\": 128,\n",
        "    \"random_state\": 42,\n",
        "    \"n_oversamples\": 20,\n",
        "    \"n_iter\": 7,\n",
        "}\n",
        "\n",
        "class IterativeSVDImputator(object):\n",
        "    def __init__(self, svd_params=default_svd_params, iters=2):\n",
        "        self.missing_values = 0.0\n",
        "        self.svd_params = svd_params\n",
        "        self.iters = iters\n",
        "        self.svd_decomposers = [None for _ in range(self.iters)]\n",
        "\n",
        "    def fit(self, X):\n",
        "        mask = X == self.missing_values\n",
        "        transformed_X = X.copy()\n",
        "        for i in range(self.iters):\n",
        "            self.svd_decomposers[i] = TruncatedSVD(**self.svd_params)\n",
        "            self.svd_decomposers[i].fit(transformed_X)\n",
        "            new_X = self.svd_decomposers[i].inverse_transform(\n",
        "                self.svd_decomposers[i].transform(transformed_X))\n",
        "            transformed_X[mask] = new_X[mask]\n",
        "\n",
        "    def transform(self, X):\n",
        "        mask = X == self.missing_values\n",
        "        transformed_X = X.copy()\n",
        "        for i in range(self.iters):\n",
        "            new_X = self.svd_decomposers[i].inverse_transform(\n",
        "                self.svd_decomposers[i].transform(transformed_X))\n",
        "            transformed_X[mask] = new_X[mask]\n",
        "        return transformed_X\n",
        "\n",
        "class scRNADataset(Dataset):\n",
        "    def __init__(self, config, dataset_name, mode='train'):\n",
        "        self.config = config\n",
        "        if mode == 'train':\n",
        "            self.iterator = self.prepare_training_pairs\n",
        "        else:\n",
        "            self.iterator = self.prepare_test_pairs\n",
        "        self.paths = config[\"paths\"]\n",
        "        self.dataset_name = dataset_name\n",
        "        self.data_path = os.path.join(self.paths[\"data\"], dataset_name)\n",
        "        self.data, self.labels = self._load_data()\n",
        "        self.data_dim = self.data.shape[1]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def prepare_training_pairs(self, idx):\n",
        "        sample = self.data[idx]\n",
        "        sample_tensor = torch.Tensor(sample)\n",
        "        cluster = int(self.labels[idx])\n",
        "        return sample, cluster\n",
        "\n",
        "    def prepare_test_pairs(self, idx):\n",
        "        sample = self.data[idx]\n",
        "        cluster = int(self.labels[idx])\n",
        "        return sample, cluster\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.iterator(index)\n",
        "\n",
        "    def _load_data(self):\n",
        "        data, labels = self.load_data(self.data_path)\n",
        "        n_classes = len(list(set(labels.reshape(-1, ).tolist())))\n",
        "        self.config[\"feat_dim\"] = data.shape[1]\n",
        "        if self.config[\"n_classes\"] != n_classes:\n",
        "            self.config[\"n_classes\"] = n_classes\n",
        "            print(f\"{50 * '>'} Number of classes changed \"\n",
        "                  f\"from {self.config['n_classes']} to {n_classes} {50 * '<'}\")\n",
        "        self.data_max = np.max(np.abs(data))\n",
        "        self.data_min = np.min(np.abs(data))\n",
        "        return data, labels\n",
        "\n",
        "    def load_data(self, path):\n",
        "        data_mat = h5py.File(f\"{path}.h5\", \"r\")\n",
        "        X = np.array(data_mat['X'])\n",
        "        Y = np.array(data_mat['Y'])\n",
        "\n",
        "        if Y.dtype != \"int64\":\n",
        "            encoder_x = LabelEncoder()\n",
        "            Y = encoder_x.fit_transform(Y)\n",
        "\n",
        "        nb_genes = 1000\n",
        "        X = np.ceil(X).astype(int)\n",
        "        count_X = X\n",
        "        print(X.shape, count_X.shape, f\"keeping {nb_genes} genes\")\n",
        "        adata = sc.AnnData(X)\n",
        "\n",
        "        adata = self.normalize(adata,\n",
        "                               copy=True,\n",
        "                               highly_genes=nb_genes,\n",
        "                               size_factors=True,\n",
        "                               normalize_input=True,\n",
        "                               logtrans_input=True)\n",
        "        sorted_genes = adata.var_names[np.argsort(adata.var[\"mean\"])]\n",
        "        adata = adata[:, sorted_genes]\n",
        "        X = adata.X.astype(np.float32)\n",
        "\n",
        "        imputator = IterativeSVDImputator(iters=2)\n",
        "        imputator.fit(X)\n",
        "        X = imputator.transform(X)\n",
        "\n",
        "        return X, Y\n",
        "\n",
        "    def normalize(self, adata, copy=True, highly_genes=None, filter_min_counts=True,\n",
        "                  size_factors=True, normalize_input=True, logtrans_input=True):\n",
        "        if isinstance(adata, sc.AnnData):\n",
        "            if copy:\n",
        "                adata = adata.copy()\n",
        "        elif isinstance(adata, str):\n",
        "            adata = sc.read(adata)\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "        norm_error = 'Make sure that the dataset (adata.X) contains unnormalized count data.'\n",
        "        assert 'n_count' not in adata.obs, norm_error\n",
        "        if adata.X.size < 50e6:\n",
        "            if sp.sparse.issparse(adata.X):\n",
        "                assert (adata.X.astype(int) != adata.X).nnz == 0, norm_error\n",
        "            else:\n",
        "                assert np.all(adata.X.astype(int) == adata.X), norm_error\n",
        "\n",
        "        if filter_min_counts:\n",
        "            sc.pp.filter_genes(adata, min_counts=1)\n",
        "            sc.pp.filter_cells(adata, min_counts=1)\n",
        "        if size_factors or normalize_input or logtrans_input:\n",
        "            adata.raw = adata.copy()\n",
        "        else:\n",
        "            adata.raw = adata\n",
        "        if size_factors:\n",
        "            sc.pp.normalize_total(adata)\n",
        "            adata.obs['size_factors'] = adata.obs.n_counts / \\\n",
        "                np.median(adata.obs.n_counts)\n",
        "        else:\n",
        "            adata.obs['size_factors'] = 1.0\n",
        "        if logtrans_input:\n",
        "            sc.pp.log1p(adata)\n",
        "        if highly_genes != None:\n",
        "            sc.pp.highly_variable_genes(\n",
        "                adata, min_mean=0.0125, max_mean=3, min_disp=0.5, n_top_genes=highly_genes, subset=True)\n",
        "        if normalize_input:\n",
        "            sc.pp.scale(adata)\n",
        "        return adata\n",
        "\n",
        "class Loader(object):\n",
        "    def __init__(self, config, dataset_name, drop_last=True, kwargs={}):\n",
        "        batch_size = config[\"batch_size\"]\n",
        "        self.config = config\n",
        "        train_dataset, test_dataset = self.get_dataset(dataset_name)\n",
        "        self.data_max = train_dataset.data_max\n",
        "        self.data_min = train_dataset.data_min\n",
        "\n",
        "        self.train_loader = DataLoader(\n",
        "            train_dataset, batch_size=batch_size, shuffle=True, drop_last=drop_last, **kwargs)\n",
        "        self.test_loader = DataLoader(\n",
        "            test_dataset, batch_size=batch_size*5, shuffle=False, drop_last=False, **kwargs)\n",
        "\n",
        "    def get_dataset(self, dataset_name):\n",
        "        loader_map = {'default_loader': scRNADataset}\n",
        "        dataset = loader_map[dataset_name] if dataset_name in loader_map.keys() else loader_map['default_loader']\n",
        "        train_dataset = dataset(self.config, dataset_name=dataset_name, mode='train')\n",
        "        test_dataset = dataset(self.config, dataset_name=dataset_name, mode='test')\n",
        "        return train_dataset, test_dataset\n",
        "\n",
        "# Evaluation Functions\n",
        "def evaluate(label, pred):\n",
        "    nmi = normalized_mutual_info_score(label, pred)\n",
        "    ari = adjusted_rand_score(label, pred)\n",
        "    pred_adjusted = get_y_preds(label, pred, max(len(set(label)), len(set(pred))))\n",
        "    acc = accuracy_score(pred_adjusted, label)\n",
        "    return nmi, ari, acc\n",
        "\n",
        "def calculate_cost_matrix(C, n_clusters):\n",
        "    cost_matrix = np.zeros((n_clusters, n_clusters))\n",
        "    for j in range(n_clusters):\n",
        "        s = np.sum(C[:, j])\n",
        "        for i in range(n_clusters):\n",
        "            t = C[i, j]\n",
        "            cost_matrix[j, i] = s - t\n",
        "    return cost_matrix\n",
        "\n",
        "def get_cluster_labels_from_indices(indices):\n",
        "    n_clusters = len(indices)\n",
        "    cluster_labels = np.zeros(n_clusters)\n",
        "    for i in range(n_clusters):\n",
        "        cluster_labels[i] = indices[i][1]\n",
        "    return cluster_labels\n",
        "\n",
        "def get_y_preds(y_true, cluster_assignments, n_clusters):\n",
        "    confusion_matrix = metrics.confusion_matrix(y_true, cluster_assignments, labels=None)\n",
        "    cost_matrix = calculate_cost_matrix(confusion_matrix, n_clusters)\n",
        "    indices = Munkres().compute(cost_matrix)\n",
        "    kmeans_to_true_cluster_labels = get_cluster_labels_from_indices(indices)\n",
        "\n",
        "    if np.min(cluster_assignments) != 0:\n",
        "        cluster_assignments = cluster_assignments - np.min(cluster_assignments)\n",
        "    y_pred = kmeans_to_true_cluster_labels[cluster_assignments]\n",
        "    return y_pred\n",
        "\n",
        "# Helper Functions\n",
        "def apply_noise(X, p=[0.2,0.4]):\n",
        "    p = torch.tensor(p)\n",
        "    should_swap = torch.bernoulli(p.to(X.device) * torch.ones((X.shape)).to(X.device))\n",
        "    corrupted_X = torch.where(should_swap == 1, X[torch.randperm(X.shape[0])], X)\n",
        "    masked = (corrupted_X != X).float()\n",
        "    return corrupted_X, masked\n",
        "\n",
        "def make_dir(directory_path, new_folder_name):\n",
        "    directory_path = os.path.join(directory_path, new_folder_name)\n",
        "    if not os.path.exists(directory_path):\n",
        "        os.makedirs(directory_path)\n",
        "    return directory_path\n",
        "\n",
        "def inference(net, data_loader_test):\n",
        "    net.eval()\n",
        "    feature_vector = []\n",
        "    labels_vector = []\n",
        "    with torch.no_grad():\n",
        "        for step, (x, y) in enumerate(data_loader_test):\n",
        "\n",
        "            feature_vector.extend(net.feature(x.to(device)).detach().cpu().numpy())\n",
        "\n",
        "            labels_vector.extend(y.numpy())\n",
        "    feature_vector = np.array(feature_vector)\n",
        "    labels_vector = np.array(labels_vector)\n",
        "    return feature_vector, labels_vector\n",
        "\n",
        "def res_search_fixed_clus(adata, fixed_clus_count, increment=0.02):\n",
        "    dis = []\n",
        "    resolutions = sorted(list(np.arange(0.01, 2.5, increment)), reverse=True)\n",
        "    i = 0\n",
        "    res_new = []\n",
        "    for res in resolutions:\n",
        "        sc.tl.leiden(adata, random_state=0, resolution=res)\n",
        "        count_unique_leiden = len(pd.DataFrame(adata.obs['leiden']).leiden.unique())\n",
        "        dis.append(abs(count_unique_leiden-fixed_clus_count))\n",
        "        res_new.append(res)\n",
        "        if count_unique_leiden == fixed_clus_count:\n",
        "            break\n",
        "    reso = resolutions[np.argmin(dis)]\n",
        "    return reso\n",
        "\n",
        "# Main Training Function\n",
        "def train(args):\n",
        "    data_load = Loader(args, dataset_name=args[\"dataset\"], drop_last=True)\n",
        "    data_loader = data_load.train_loader\n",
        "    data_loader_test = data_load.test_loader\n",
        "    x_shape = args[\"data_dim\"]\n",
        "\n",
        "    results = []\n",
        "    init_lr = args[\"learning_rate\"]\n",
        "    max_epochs = args[\"epochs\"]\n",
        "    mask_probas = [0.2]*x_shape\n",
        "\n",
        "\n",
        "    model = AutoEncoder(\n",
        "    num_genes=x_shape,\n",
        "    hidden_size=128,\n",
        "    masked_data_weight=0.75,\n",
        "    mask_loss_weight=0.7\n",
        ").to(device)\n",
        "\n",
        "    model_checkpoint = 'model_checkpoint.pth'\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=init_lr)\n",
        "\n",
        "    for epoch in range(max_epochs):\n",
        "        model.train()\n",
        "        meter = AverageMeter()\n",
        "        for i, (x, y) in enumerate(data_loader):\n",
        "            x = x.to(device)\n",
        "            x_corrputed, mask = apply_noise(x, mask_probas)\n",
        "            optimizer.zero_grad()\n",
        "            x_corrputed_latent, loss_ae = model.loss_mask(x_corrputed, x, mask)\n",
        "            loss_ae.backward()\n",
        "            optimizer.step()\n",
        "            meter.update(loss_ae.detach().cpu().numpy())\n",
        "\n",
        "        if epoch == 80:\n",
        "            latent, true_label = inference(model, data_loader_test)\n",
        "            if latent.shape[0] < 10000:\n",
        "                clustering_model = KMeans(n_clusters=args[\"n_classes\"])\n",
        "                clustering_model.fit(latent)\n",
        "                pred_label = clustering_model.labels_\n",
        "            else:\n",
        "                adata = sc.AnnData(latent)\n",
        "                sc.pp.neighbors(adata, n_neighbors=10, use_rep=\"X\")\n",
        "                reso = res_search_fixed_clus(adata, args[\"n_classes\"])\n",
        "                sc.tl.leiden(adata, resolution=reso)\n",
        "                pred = adata.obs['leiden'].to_list()\n",
        "                pred_label = [int(x) for x in pred]\n",
        "\n",
        "            nmi, ari, acc = evaluate(true_label, pred_label)\n",
        "            ss = silhouette_score(latent, pred_label)\n",
        "\n",
        "            res = {}\n",
        "            res[\"nmi\"] = nmi\n",
        "            res[\"ari\"] = ari\n",
        "            res[\"acc\"] = acc\n",
        "            res[\"sil\"] = ss\n",
        "            res[\"dataset\"] = args[\"dataset\"]\n",
        "            res[\"epoch\"] = epoch\n",
        "            results.append(res)\n",
        "\n",
        "            print(\"\\tEvalute: [nmi: %f] [ari: %f] [acc: %f]\" % (nmi, ari, acc))\n",
        "\n",
        "            np.save(args[\"save_path\"]+\"/embedding_\"+str(epoch)+\".npy\", latent)\n",
        "            pd.DataFrame({\"True\": true_label, \"Pred\": pred_label}).to_csv(args[\"save_path\"]+\"/types_\"+str(epoch)+\".txt\")\n",
        "\n",
        "    torch.save({\"optimizer\": optimizer.state_dict(), \"model\": model.state_dict()}, model_checkpoint)\n",
        "    return results\n",
        "\n",
        "# Main Execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Set random seeds for reproducibility\n",
        "    seed = 42\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    # Configuration\n",
        "    args = {\n",
        "        \"num_workers\": 4,\n",
        "        \"paths\": {\n",
        "            \"data\": \".\",  # Assuming Pollen.h5 is in the current directory\n",
        "            \"results\": \"./results\"\n",
        "        },\n",
        "        'batch_size': 256,\n",
        "        \"data_dim\": 1000,\n",
        "        'n_classes': 7,  # Adjust based on your dataset\n",
        "        'epochs': 100,\n",
        "        \"dataset\": \"Pollen\",  # Using your input file name without .h5 extension\n",
        "        \"learning_rate\": 1e-3,\n",
        "        \"latent_dim\": 32,\n",
        "        \"save_path\": \"./results/Pollen\"  # Output directory\n",
        "    }\n",
        "\n",
        "    # Create output directory\n",
        "    os.makedirs(args[\"save_path\"], exist_ok=True)\n",
        "    os.makedirs(args[\"paths\"][\"results\"], exist_ok=True)\n",
        "\n",
        "    print(\"Starting training with configuration:\")\n",
        "    print(args)\n",
        "\n",
        "    # Run training\n",
        "    results = train(args)\n",
        "\n",
        "    # Save final results\n",
        "    results_df = pd.DataFrame(results)\n",
        "    results_df.to_csv(os.path.join(args[\"paths\"][\"results\"], \"final_results.csv\"))\n",
        "    print(\"Training completed and results saved.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dg-Ab9qGiC0F",
        "outputId": "886ea622-4985-40f8-b7c6-27acd0544297"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training with configuration:\n",
            "{'num_workers': 4, 'paths': {'data': '.', 'results': './results'}, 'batch_size': 256, 'data_dim': 1000, 'n_classes': 5, 'epochs': 100, 'dataset': 'Pollen', 'learning_rate': 0.001, 'latent_dim': 32, 'save_path': './results/Pollen'}\n",
            "(301, 21721) (301, 21721) keeping 1000 genes\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> Number of classes changed from 11 to 11 <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "(301, 21721) (301, 21721) keeping 1000 genes\n",
            "Epoch 0/100, Loss: 0.6513\n",
            "Epoch 10/100, Loss: 0.5041\n",
            "Epoch 20/100, Loss: 0.4340\n",
            "Epoch 30/100, Loss: 0.4090\n",
            "Epoch 40/100, Loss: 0.4101\n",
            "Epoch 50/100, Loss: 0.4020\n",
            "Epoch 60/100, Loss: 0.4102\n",
            "Epoch 70/100, Loss: 0.3964\n",
            "Epoch 80/100, Loss: 0.4008\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/manifold/_spectral_embedding.py:329: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluation Results at Epoch 80:\n",
            "============================================================\n",
            "Method          | NMI      | ARI      | Accuracy\n",
            "------------------------------------------------------------\n",
            "KMeans          | 0.9105 | 0.9092 | 0.8837\n",
            "DBSCAN          | 0.7823 | 0.5198 | 0.0897\n",
            "GMM             | 0.9105 | 0.9092 | 0.8837\n",
            "Spectral        | 0.8255 | 0.6466 | 0.7475\n",
            "Epoch 90/100, Loss: 0.3965\n",
            "Epoch 99/100, Loss: 0.3909\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/manifold/_spectral_embedding.py:329: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluation Results at Epoch 99:\n",
            "============================================================\n",
            "Method          | NMI      | ARI      | Accuracy\n",
            "------------------------------------------------------------\n",
            "KMeans          | 0.9256 | 0.9301 | 0.9203\n",
            "DBSCAN          | 0.7805 | 0.5147 | 0.0797\n",
            "GMM             | 0.9256 | 0.9301 | 0.9203\n",
            "Spectral        | 0.8209 | 0.6351 | 0.6811\n",
            "\n",
            "Final Results Summary:\n",
            "============================================================\n",
            "Method          | NMI      | ARI      | Accuracy\n",
            "------------------------------------------------------------\n",
            "KMeans          | 0.9256 | 0.9301 | 0.9203\n",
            "DBSCAN          | 0.7823 | 0.5198 | 0.0897\n",
            "GMM             | 0.9256 | 0.9301 | 0.9203\n",
            "Spectral        | 0.8255 | 0.6466 | 0.7475\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import random\n",
        "import h5py\n",
        "import scanpy as sc\n",
        "import scipy as sp\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn.functional import binary_cross_entropy_with_logits as bce_logits\n",
        "from torch.nn.functional import mse_loss as mse\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.cluster import KMeans, DBSCAN, SpectralClustering\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.metrics import silhouette_score, normalized_mutual_info_score, adjusted_rand_score, accuracy_score\n",
        "from scipy.optimize import linear_sum_assignment as hungarian\n",
        "from sklearn import metrics\n",
        "from munkres import Munkres\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set CUDA device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Utility Classes and Functions\n",
        "class AverageMeter(object):\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "    def reset(self):\n",
        "        self.val, self.avg, self.sum, self.count = 0, 0, 0, 0\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "def evaluate(label, pred):\n",
        "    nmi = normalized_mutual_info_score(label, pred)\n",
        "    ari = adjusted_rand_score(label, pred)\n",
        "    pred_adjusted = get_y_preds(label, pred, max(len(set(label)), len(set(pred))))\n",
        "    acc = accuracy_score(pred_adjusted, label)\n",
        "    return nmi, ari, acc\n",
        "\n",
        "def get_y_preds(y_true, cluster_assignments, n_clusters):\n",
        "    confusion_matrix = metrics.confusion_matrix(y_true, cluster_assignments, labels=None)\n",
        "    cost_matrix = calculate_cost_matrix(confusion_matrix, n_clusters)\n",
        "    indices = Munkres().compute(cost_matrix)\n",
        "    kmeans_to_true_cluster_labels = get_cluster_labels_from_indices(indices)\n",
        "    if np.min(cluster_assignments) != 0:\n",
        "        cluster_assignments = cluster_assignments - np.min(cluster_assignments)\n",
        "    y_pred = kmeans_to_true_cluster_labels[cluster_assignments]\n",
        "    return y_pred\n",
        "\n",
        "def calculate_cost_matrix(C, n_clusters):\n",
        "    cost_matrix = np.zeros((n_clusters, n_clusters))\n",
        "    for j in range(n_clusters):\n",
        "        s = np.sum(C[:, j])\n",
        "        for i in range(n_clusters):\n",
        "            t = C[i, j]\n",
        "            cost_matrix[j, i] = s - t\n",
        "    return cost_matrix\n",
        "\n",
        "def get_cluster_labels_from_indices(indices):\n",
        "    n_clusters = len(indices)\n",
        "    cluster_labels = np.zeros(n_clusters)\n",
        "    for i in range(n_clusters):\n",
        "        cluster_labels[i] = indices[i][1]\n",
        "    return cluster_labels\n",
        "\n",
        "def cluster_with_methods(latent, true_label, n_classes):\n",
        "    results = {}\n",
        "\n",
        "    # 1: K-Means\n",
        "    kmeans = KMeans(n_clusters=n_classes, random_state=42)\n",
        "    pred_kmeans = kmeans.fit_predict(latent)\n",
        "    nmi, ari, acc = evaluate(true_label, pred_kmeans)\n",
        "    results['KMeans'] = {'nmi': nmi, 'ari': ari, 'acc': acc, 'labels': pred_kmeans}\n",
        "\n",
        "\n",
        "    # 2: DBSCAN\n",
        "    neigh = NearestNeighbors(n_neighbors=2)\n",
        "    nbrs = neigh.fit(latent)\n",
        "    distances, indices = nbrs.kneighbors(latent)\n",
        "    distances = np.sort(distances, axis=0)\n",
        "    distances = distances[:,1]\n",
        "    eps = distances[int(0.95*len(distances))]\n",
        "\n",
        "    dbscan = DBSCAN(eps=eps, min_samples=5)\n",
        "    pred_dbscan = dbscan.fit_predict(latent)\n",
        "\n",
        "    if len(set(pred_dbscan)) <= 1:\n",
        "        pred_dbscan = kmeans.fit_predict(latent)\n",
        "\n",
        "    nmi, ari, acc = evaluate(true_label, pred_dbscan)\n",
        "    results['DBSCAN'] = {'nmi': nmi, 'ari': ari, 'acc': acc, 'labels': pred_dbscan}\n",
        "\n",
        "    # 3: Gaussian Mixture Model\n",
        "    gmm = GaussianMixture(n_components=n_classes, random_state=42)\n",
        "    pred_gmm = gmm.fit_predict(latent)\n",
        "    nmi, ari, acc = evaluate(true_label, pred_gmm)\n",
        "    results['GMM'] = {'nmi': nmi, 'ari': ari, 'acc': acc, 'labels': pred_gmm}\n",
        "\n",
        "    # 4: Spectral Clustering\n",
        "    spectral = SpectralClustering(n_clusters=n_classes, random_state=42, affinity='nearest_neighbors')\n",
        "    pred_spectral = spectral.fit_predict(latent)\n",
        "    nmi, ari, acc = evaluate(true_label, pred_spectral)\n",
        "    results['Spectral'] = {'nmi': nmi, 'ari': ari, 'acc': acc, 'labels': pred_spectral}\n",
        "\n",
        "    return results\n",
        "\n",
        "# Model and Data Classes\n",
        "class AutoEncoder(torch.nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_genes,\n",
        "        hidden_size=128,\n",
        "        dropout=0,\n",
        "        masked_data_weight=.75,\n",
        "        mask_loss_weight=0.7,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.num_genes = num_genes\n",
        "        self.masked_data_weight = masked_data_weight\n",
        "        self.mask_loss_weight = mask_loss_weight\n",
        "\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Dropout(p=dropout),\n",
        "            nn.Linear(self.num_genes, 256),\n",
        "            nn.LayerNorm(256),\n",
        "            nn.Mish(inplace=True),\n",
        "            nn.Linear(256, hidden_size),\n",
        "            nn.LayerNorm(hidden_size),\n",
        "            nn.Mish(inplace=True),\n",
        "            nn.Linear(hidden_size, hidden_size)\n",
        "        )\n",
        "\n",
        "        self.mask_predictor = nn.Linear(hidden_size, num_genes)\n",
        "        self.decoder = nn.Linear(\n",
        "            in_features=hidden_size+num_genes, out_features=num_genes)\n",
        "\n",
        "    def forward_mask(self, x):\n",
        "        latent = self.encoder(x)\n",
        "        predicted_mask = self.mask_predictor(latent)\n",
        "        reconstruction = self.decoder(\n",
        "            torch.cat([latent, predicted_mask], dim=1))\n",
        "        return latent, predicted_mask, reconstruction\n",
        "\n",
        "    def loss_mask(self, x, y, mask):\n",
        "        latent, predicted_mask, reconstruction = self.forward_mask(x)\n",
        "        w_nums = mask * self.masked_data_weight + (1 - mask) * (1 - self.masked_data_weight)\n",
        "        reconstruction_loss = (1-self.mask_loss_weight) * torch.mul(\n",
        "            w_nums, mse(reconstruction, y, reduction='none'))\n",
        "        mask_loss = self.mask_loss_weight * \\\n",
        "            bce_logits(predicted_mask, mask, reduction=\"mean\")\n",
        "        reconstruction_loss = reconstruction_loss.mean()\n",
        "        loss = reconstruction_loss + mask_loss\n",
        "        return latent, loss\n",
        "\n",
        "    def feature(self, x):\n",
        "        latent = self.encoder(x)\n",
        "        return latent\n",
        "\n",
        "# Data Processing Classes\n",
        "default_svd_params = {\n",
        "    \"n_components\": 128,\n",
        "    \"random_state\": 42,\n",
        "    \"n_oversamples\": 20,\n",
        "    \"n_iter\": 7,\n",
        "}\n",
        "\n",
        "class scRNADataset(Dataset):\n",
        "    def __init__(self, config, dataset_name, mode='train'):\n",
        "        self.config = config\n",
        "        if mode == 'train':\n",
        "            self.iterator = self.prepare_training_pairs\n",
        "        else:\n",
        "            self.iterator = self.prepare_test_pairs\n",
        "        self.paths = config[\"paths\"]\n",
        "        self.dataset_name = dataset_name\n",
        "        self.data_path = os.path.join(self.paths[\"data\"], dataset_name)\n",
        "        self.data, self.labels = self._load_data()\n",
        "        self.data_dim = self.data.shape[1]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def prepare_training_pairs(self, idx):\n",
        "        sample = self.data[idx]\n",
        "        sample_tensor = torch.Tensor(sample)\n",
        "        cluster = int(self.labels[idx])\n",
        "        return sample, cluster\n",
        "\n",
        "    def prepare_test_pairs(self, idx):\n",
        "        sample = self.data[idx]\n",
        "        cluster = int(self.labels[idx])\n",
        "        return sample, cluster\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.iterator(index)\n",
        "\n",
        "    def _load_data(self):\n",
        "        data, labels = self.load_data(self.data_path)\n",
        "        n_classes = len(list(set(labels.reshape(-1, ).tolist())))\n",
        "        self.config[\"feat_dim\"] = data.shape[1]\n",
        "        if self.config[\"n_classes\"] != n_classes:\n",
        "            self.config[\"n_classes\"] = n_classes\n",
        "            print(f\"{50 * '>'} Number of classes changed \"\n",
        "                  f\"from {self.config['n_classes']} to {n_classes} {50 * '<'}\")\n",
        "        self.data_max = np.max(np.abs(data))\n",
        "        self.data_min = np.min(np.abs(data))\n",
        "        return data, labels\n",
        "\n",
        "    def load_data(self, path):\n",
        "        data_mat = h5py.File(f\"{path}.h5\", \"r\")\n",
        "        X = np.array(data_mat['X'])\n",
        "        Y = np.array(data_mat['Y'])\n",
        "\n",
        "        if Y.dtype != \"int64\":\n",
        "            encoder_x = LabelEncoder()\n",
        "            Y = encoder_x.fit_transform(Y)\n",
        "\n",
        "        nb_genes = 1000\n",
        "        X = np.ceil(X).astype(int)\n",
        "        count_X = X\n",
        "        print(X.shape, count_X.shape, f\"keeping {nb_genes} genes\")\n",
        "        adata = sc.AnnData(X)\n",
        "\n",
        "        adata = self.normalize(adata,\n",
        "                               copy=True,\n",
        "                               highly_genes=nb_genes,\n",
        "                               size_factors=True,\n",
        "                               normalize_input=True,\n",
        "                               logtrans_input=True)\n",
        "        sorted_genes = adata.var_names[np.argsort(adata.var[\"mean\"])]\n",
        "        adata = adata[:, sorted_genes]\n",
        "        X = adata.X.astype(np.float32)\n",
        "\n",
        "        imputator = IterativeSVDImputator(iters=2)\n",
        "        imputator.fit(X)\n",
        "        X = imputator.transform(X)\n",
        "\n",
        "        return X, Y\n",
        "\n",
        "    def normalize(self, adata, copy=True, highly_genes=None, filter_min_counts=True,\n",
        "                  size_factors=True, normalize_input=True, logtrans_input=True):\n",
        "        if isinstance(adata, sc.AnnData):\n",
        "            if copy:\n",
        "                adata = adata.copy()\n",
        "        elif isinstance(adata, str):\n",
        "            adata = sc.read(adata)\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "        norm_error = 'Make sure that the dataset (adata.X) contains unnormalized count data.'\n",
        "        assert 'n_count' not in adata.obs, norm_error\n",
        "        if adata.X.size < 50e6:\n",
        "            if sp.sparse.issparse(adata.X):\n",
        "                assert (adata.X.astype(int) != adata.X).nnz == 0, norm_error\n",
        "            else:\n",
        "                assert np.all(adata.X.astype(int) == adata.X), norm_error\n",
        "\n",
        "        if filter_min_counts:\n",
        "            sc.pp.filter_genes(adata, min_counts=1)\n",
        "            sc.pp.filter_cells(adata, min_counts=1)\n",
        "        if size_factors or normalize_input or logtrans_input:\n",
        "            adata.raw = adata.copy()\n",
        "        else:\n",
        "            adata.raw = adata\n",
        "        if size_factors:\n",
        "            sc.pp.normalize_total(adata)\n",
        "            adata.obs['size_factors'] = adata.obs.n_counts / \\\n",
        "                np.median(adata.obs.n_counts)\n",
        "        else:\n",
        "            adata.obs['size_factors'] = 1.0\n",
        "        if logtrans_input:\n",
        "            sc.pp.log1p(adata)\n",
        "        if highly_genes != None:\n",
        "            sc.pp.highly_variable_genes(\n",
        "                adata, min_mean=0.0125, max_mean=3, min_disp=0.5, n_top_genes=highly_genes, subset=True)\n",
        "        if normalize_input:\n",
        "            sc.pp.scale(adata)\n",
        "        return adata\n",
        "\n",
        "class Loader(object):\n",
        "    def __init__(self, config, dataset_name, drop_last=True, kwargs={}):\n",
        "        batch_size = config[\"batch_size\"]\n",
        "        self.config = config\n",
        "        train_dataset, test_dataset = self.get_dataset(dataset_name)\n",
        "        self.data_max = train_dataset.data_max\n",
        "        self.data_min = train_dataset.data_min\n",
        "\n",
        "        self.train_loader = DataLoader(\n",
        "            train_dataset, batch_size=batch_size, shuffle=True, drop_last=drop_last, **kwargs)\n",
        "        self.test_loader = DataLoader(\n",
        "            test_dataset, batch_size=batch_size*5, shuffle=False, drop_last=False, **kwargs)\n",
        "\n",
        "    def get_dataset(self, dataset_name):\n",
        "        loader_map = {'default_loader': scRNADataset}\n",
        "        dataset = loader_map[dataset_name] if dataset_name in loader_map.keys() else loader_map['default_loader']\n",
        "        train_dataset = dataset(self.config, dataset_name=dataset_name, mode='train')\n",
        "        test_dataset = dataset(self.config, dataset_name=dataset_name, mode='test')\n",
        "        return train_dataset, test_dataset\n",
        "\n",
        "# train \n",
        "def train(args):\n",
        "    data_load = Loader(args, dataset_name=args[\"dataset\"], drop_last=True)\n",
        "    data_loader = data_load.train_loader\n",
        "    data_loader_test = data_load.test_loader\n",
        "    x_shape = args[\"data_dim\"]\n",
        "\n",
        "    results = []\n",
        "    init_lr = args[\"learning_rate\"]\n",
        "    max_epochs = args[\"epochs\"]\n",
        "    mask_probas = [0.6]*x_shape\n",
        "\n",
        "    model = AutoEncoder(\n",
        "        num_genes=x_shape,\n",
        "        hidden_size=128,\n",
        "        masked_data_weight=0.75,\n",
        "        mask_loss_weight=0.7\n",
        "    ).to(device)\n",
        "\n",
        "    model_checkpoint = 'model_checkpoint.pth'\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=init_lr)\n",
        "\n",
        "    for epoch in range(max_epochs):\n",
        "        model.train()\n",
        "        meter = AverageMeter()\n",
        "        for i, (x, y) in enumerate(data_loader):\n",
        "            x = x.to(device)\n",
        "            x_corrputed, mask = apply_noise(x, mask_probas)\n",
        "            optimizer.zero_grad()\n",
        "            x_corrputed_latent, loss_ae = model.loss_mask(x_corrputed, x, mask)\n",
        "            loss_ae.backward()\n",
        "            optimizer.step()\n",
        "            meter.update(loss_ae.detach().cpu().numpy())\n",
        "\n",
        "        if epoch % 10 == 0 or epoch == max_epochs - 1:\n",
        "            print(f\"Epoch {epoch}/{max_epochs}, Loss: {meter.avg:.4f}\")\n",
        "\n",
        "        if epoch == 80 or epoch == max_epochs - 1:\n",
        "            latent, true_label = inference(model, data_loader_test)\n",
        "\n",
        "            clustering_results = cluster_with_methods(latent, true_label, args[\"n_classes\"])\n",
        "\n",
        "            print(f\"\\nEvaluation Results at Epoch {epoch}:\")\n",
        "            print(\"=\"*60)\n",
        "            print(f\"{'Method':<15} | {'NMI':<8} | {'ARI':<8} | {'Accuracy':<8}\")\n",
        "            print(\"-\"*60)\n",
        "\n",
        "            for method, metrics in clustering_results.items():\n",
        "                res = {\n",
        "                    \"method\": method,\n",
        "                    \"nmi\": metrics['nmi'],\n",
        "                    \"ari\": metrics['ari'],\n",
        "                    \"acc\": metrics['acc'],\n",
        "                    \"dataset\": args[\"dataset\"],\n",
        "                    \"epoch\": epoch\n",
        "                }\n",
        "                results.append(res)\n",
        "                print(f\"{method:<15} | {metrics['nmi']:.4f} | {metrics['ari']:.4f} | {metrics['acc']:.4f}\")\n",
        "\n",
        "            # Visualization (optional)\n",
        "            plt.figure(figsize=(15, 10))\n",
        "            for i, (method, metrics) in enumerate(clustering_results.items(), 1):\n",
        "                plt.subplot(2, 3, i)\n",
        "                plt.scatter(latent[:, 0], latent[:, 1], c=metrics['labels'], cmap='tab20', s=5)\n",
        "                plt.title(f\"{method}\\nNMI: {metrics['nmi']:.2f}, ARI: {metrics['ari']:.2f}\")\n",
        "                plt.colorbar()\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(os.path.join(args[\"save_path\"], f\"clustering_results_epoch_{epoch}.png\"))\n",
        "            plt.close()\n",
        "\n",
        "            np.save(os.path.join(args[\"save_path\"], f\"embedding_{epoch}.npy\"), latent)\n",
        "            pd.DataFrame({\"True\": true_label}).to_csv(os.path.join(args[\"save_path\"], f\"true_labels_{epoch}.csv\"))\n",
        "\n",
        "    torch.save({\"optimizer\": optimizer.state_dict(), \"model\": model.state_dict()}, model_checkpoint)\n",
        "\n",
        "    # Save final results\n",
        "    results_df = pd.DataFrame(results)\n",
        "    results_df.to_csv(os.path.join(args[\"save_path\"], \"final_results.csv\"), index=False)\n",
        "\n",
        "    return results\n",
        "\n",
        "# \n",
        "if __name__ == \"__main__\":\n",
        "    # Set random seeds\n",
        "    seed = 42\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "    # Configuration\n",
        "    args = {\n",
        "        \"num_workers\": 4,\n",
        "        \"paths\": {\n",
        "            \"data\": \".\",\n",
        "            \"results\": \"./results\"\n",
        "        },\n",
        "        'batch_size': 256,\n",
        "        \"data_dim\": 1000,\n",
        "        'n_classes': 5,\n",
        "        'epochs': 100,\n",
        "        \"dataset\": \"Pollen\",\n",
        "        \"learning_rate\": 1e-3,\n",
        "        \"latent_dim\": 32,\n",
        "        \"save_path\": \"./results/Pollen\"\n",
        "    }\n",
        "\n",
        "    os.makedirs(args[\"save_path\"], exist_ok=True)\n",
        "    os.makedirs(args[\"paths\"][\"results\"], exist_ok=True)\n",
        "\n",
        "    print(\"Starting training with configuration:\")\n",
        "    print(args)\n",
        "\n",
        "    results = train(args)\n",
        "\n",
        "    print(\"\\nFinal Results Summary:\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"{'Method':<15} | {'NMI':<8} | {'ARI':<8} | {'Accuracy':<8}\")\n",
        "    print(\"-\"*60)\n",
        "\n",
        "    final_results = pd.read_csv(os.path.join(args[\"save_path\"], \"final_results.csv\"))\n",
        "    for method in ['KMeans', 'DBSCAN', 'GMM', 'Spectral']:\n",
        "        method_results = final_results[final_results['method'] == method]\n",
        "        best_idx = method_results['nmi'].idxmax()\n",
        "        best = method_results.loc[best_idx]\n",
        "        print(f\"{method:<15} | {best['nmi']:.4f} | {best['ari']:.4f} | {best['acc']:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import h5py\n",
        "import scanpy as sc\n",
        "import scipy as sp\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn.functional import binary_cross_entropy_with_logits as bce_logits\n",
        "from torch.nn.functional import mse_loss as mse\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from sklearn.metrics import silhouette_score, normalized_mutual_info_score, adjusted_rand_score, accuracy_score\n",
        "from scipy.optimize import linear_sum_assignment as hungarian\n",
        "from sklearn import metrics\n",
        "from munkres import Munkres\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set CUDA device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Utility Classes and Functions\n",
        "class AverageMeter(object):\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "    def reset(self):\n",
        "        self.val, self.avg, self.sum, self.count = 0, 0, 0, 0\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "def evaluate(label, pred):\n",
        "    nmi = normalized_mutual_info_score(label, pred)\n",
        "    ari = adjusted_rand_score(label, pred)\n",
        "    pred_adjusted = get_y_preds(label, pred, max(len(set(label)), len(set(pred))))\n",
        "    acc = accuracy_score(pred_adjusted, label)\n",
        "    return nmi, ari, acc\n",
        "\n",
        "def get_y_preds(y_true, cluster_assignments, n_clusters):\n",
        "    confusion_matrix = metrics.confusion_matrix(y_true, cluster_assignments, labels=None)\n",
        "    cost_matrix = calculate_cost_matrix(confusion_matrix, n_clusters)\n",
        "    indices = Munkres().compute(cost_matrix)\n",
        "    kmeans_to_true_cluster_labels = get_cluster_labels_from_indices(indices)\n",
        "    if np.min(cluster_assignments) != 0:\n",
        "        cluster_assignments = cluster_assignments - np.min(cluster_assignments)\n",
        "    y_pred = kmeans_to_true_cluster_labels[cluster_assignments]\n",
        "    return y_pred\n",
        "\n",
        "def calculate_cost_matrix(C, n_clusters):\n",
        "    cost_matrix = np.zeros((n_clusters, n_clusters))\n",
        "    for j in range(n_clusters):\n",
        "        s = np.sum(C[:, j])\n",
        "        for i in range(n_clusters):\n",
        "            t = C[i, j]\n",
        "            cost_matrix[j, i] = s - t\n",
        "    return cost_matrix\n",
        "\n",
        "def get_cluster_labels_from_indices(indices):\n",
        "    n_clusters = len(indices)\n",
        "    cluster_labels = np.zeros(n_clusters)\n",
        "    for i in range(n_clusters):\n",
        "        cluster_labels[i] = indices[i][1]\n",
        "    return cluster_labels\n",
        "\n",
        "def cluster_with_methods(latent, true_label, n_classes):\n",
        "    results = {}\n",
        "\n",
        "    # Agglomerative Clustering\n",
        "    agg = AgglomerativeClustering(n_clusters=n_classes)\n",
        "    pred_agg = agg.fit_predict(latent)\n",
        "    nmi, ari, acc = evaluate(true_label, pred_agg)\n",
        "    results['Agglomerative'] = {'nmi': nmi, 'ari': ari, 'acc': acc, 'labels': pred_agg}\n",
        "\n",
        "    return results\n",
        "\n",
        "class AttentionLayer(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super().__init__()\n",
        "        self.query = nn.Linear(input_dim, input_dim)\n",
        "        self.key = nn.Linear(input_dim, input_dim)\n",
        "        self.value = nn.Linear(input_dim, input_dim)\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        Q = self.query(x)\n",
        "        K = self.key(x)\n",
        "        V = self.value(x)\n",
        "\n",
        "        attention_scores = torch.matmul(Q, K.transpose(-2, -1)) / (x.size(-1) ** 0.5)\n",
        "        attention_weights = self.softmax(attention_scores)\n",
        "        output = torch.matmul(attention_weights, V)\n",
        "        return output\n",
        "\n",
        "class DeepAttentionEncoder(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.attention1 = AttentionLayer(input_dim)\n",
        "        self.norm1 = nn.LayerNorm(input_dim)\n",
        "        self.ffn1 = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(hidden_dim, input_dim)\n",
        "        )\n",
        "        self.norm2 = nn.LayerNorm(input_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        attn_out = self.attention1(x)\n",
        "        x = self.norm1(x + attn_out)\n",
        "        ffn_out = self.ffn1(x)\n",
        "        x = self.norm2(x + ffn_out)\n",
        "        return x\n",
        "\n",
        "class AutoEncoder(torch.nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_genes,\n",
        "        hidden_size=128,\n",
        "        dropout=0.6,\n",
        "        masked_data_weight=.75,\n",
        "        mask_loss_weight=0.7,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.num_genes = num_genes\n",
        "        self.masked_data_weight = masked_data_weight\n",
        "        self.mask_loss_weight = mask_loss_weight\n",
        "\n",
        "        # Enhanced Encoder with Deep Attention\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Dropout(p=dropout),\n",
        "            nn.Linear(self.num_genes, 256),\n",
        "            nn.LayerNorm(256),\n",
        "            nn.GELU(),\n",
        "            DeepAttentionEncoder(256, 512),\n",
        "            nn.Linear(256, hidden_size),\n",
        "            nn.LayerNorm(hidden_size),\n",
        "            nn.GELU(),\n",
        "            DeepAttentionEncoder(hidden_size, hidden_size * 2),\n",
        "            nn.Linear(hidden_size, hidden_size)\n",
        "        )\n",
        "\n",
        "        # Enhanced Mask Predictor with Attention\n",
        "        self.mask_predictor = nn.Sequential(\n",
        "            DeepAttentionEncoder(hidden_size, hidden_size*4),\n",
        "            nn.Linear(hidden_size, num_genes)\n",
        "        )\n",
        "\n",
        "        # Enhanced Decoder with Attention\n",
        "        self.decoder = nn.Sequential(\n",
        "            DeepAttentionEncoder(hidden_size+num_genes, (hidden_size+num_genes)*2),\n",
        "            nn.Linear(hidden_size+num_genes, num_genes)\n",
        "        )\n",
        "\n",
        "    def forward_mask(self, x):\n",
        "        latent = self.encoder(x)\n",
        "        predicted_mask = self.mask_predictor(latent)\n",
        "        reconstruction = self.decoder(\n",
        "            torch.cat([latent, predicted_mask], dim=1))\n",
        "        return latent, predicted_mask, reconstruction\n",
        "\n",
        "    def loss_mask(self, x, y, mask):\n",
        "        latent, predicted_mask, reconstruction = self.forward_mask(x)\n",
        "        w_nums = mask * self.masked_data_weight + (1 - mask) * (1 - self.masked_data_weight)\n",
        "        reconstruction_loss = (1-self.mask_loss_weight) * torch.mul(\n",
        "            w_nums, mse(reconstruction, y, reduction='none'))\n",
        "        mask_loss = self.mask_loss_weight * \\\n",
        "            bce_logits(predicted_mask, mask, reduction=\"mean\")\n",
        "        reconstruction_loss = reconstruction_loss.mean()\n",
        "        loss = reconstruction_loss + mask_loss\n",
        "        return latent, loss\n",
        "\n",
        "    def feature(self, x):\n",
        "        latent = self.encoder(x)\n",
        "        return latent\n",
        "\n",
        "# Data Processing Classes\n",
        "default_svd_params = {\n",
        "    \"n_components\": 128,\n",
        "    \"random_state\": 42,\n",
        "    \"n_oversamples\": 20,\n",
        "    \"n_iter\": 7,\n",
        "}\n",
        "\n",
        "class scRNADataset(Dataset):\n",
        "    def __init__(self, config, dataset_name, mode='train'):\n",
        "        self.config = config\n",
        "        if mode == 'train':\n",
        "            self.iterator = self.prepare_training_pairs\n",
        "        else:\n",
        "            self.iterator = self.prepare_test_pairs\n",
        "        self.paths = config[\"paths\"]\n",
        "        self.dataset_name = dataset_name\n",
        "        self.data_path = os.path.join(self.paths[\"data\"], dataset_name)\n",
        "        self.data, self.labels = self._load_data()\n",
        "        self.data_dim = self.data.shape[1]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def prepare_training_pairs(self, idx):\n",
        "        sample = self.data[idx]\n",
        "        sample_tensor = torch.Tensor(sample)\n",
        "        cluster = int(self.labels[idx])\n",
        "        return sample, cluster\n",
        "\n",
        "    def prepare_test_pairs(self, idx):\n",
        "        sample = self.data[idx]\n",
        "        cluster = int(self.labels[idx])\n",
        "        return sample, cluster\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.iterator(index)\n",
        "\n",
        "    def _load_data(self):\n",
        "        data, labels = self.load_data(self.data_path)\n",
        "        n_classes = len(list(set(labels.reshape(-1, ).tolist())))\n",
        "        self.config[\"feat_dim\"] = data.shape[1]\n",
        "        if self.config[\"n_classes\"] != n_classes:\n",
        "            self.config[\"n_classes\"] = n_classes\n",
        "            print(f\"{50 * '>'} Number of classes changed \"\n",
        "                  f\"from {self.config['n_classes']} to {n_classes} {50 * '<'}\")\n",
        "        self.data_max = np.max(np.abs(data))\n",
        "        self.data_min = np.min(np.abs(data))\n",
        "        return data, labels\n",
        "\n",
        "    def load_data(self, path):\n",
        "        data_mat = h5py.File(f\"{path}.h5\", \"r\")\n",
        "        X = np.array(data_mat['X'])\n",
        "        Y = np.array(data_mat['Y'])\n",
        "\n",
        "        if Y.dtype != \"int64\":\n",
        "            encoder_x = LabelEncoder()\n",
        "            Y = encoder_x.fit_transform(Y)\n",
        "\n",
        "        nb_genes = 1000\n",
        "        X = np.ceil(X).astype(int)\n",
        "        count_X = X\n",
        "        print(X.shape, count_X.shape, f\"keeping {nb_genes} genes\")\n",
        "        adata = sc.AnnData(X)\n",
        "\n",
        "        adata = self.normalize(adata,\n",
        "                               copy=True,\n",
        "                               highly_genes=nb_genes,\n",
        "                               size_factors=True,\n",
        "                               normalize_input=True,\n",
        "                               logtrans_input=True)\n",
        "        sorted_genes = adata.var_names[np.argsort(adata.var[\"mean\"])]\n",
        "        adata = adata[:, sorted_genes]\n",
        "        X = adata.X.astype(np.float32)\n",
        "\n",
        "        imputator = IterativeSVDImputator(iters=2)\n",
        "        imputator.fit(X)\n",
        "        X = imputator.transform(X)\n",
        "\n",
        "        return X, Y\n",
        "\n",
        "    def normalize(self, adata, copy=True, highly_genes=None, filter_min_counts=True,\n",
        "                  size_factors=True, normalize_input=True, logtrans_input=True):\n",
        "        if isinstance(adata, sc.AnnData):\n",
        "            if copy:\n",
        "                adata = adata.copy()\n",
        "        elif isinstance(adata, str):\n",
        "            adata = sc.read(adata)\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "        norm_error = 'Make sure that the dataset (adata.X) contains unnormalized count data.'\n",
        "        assert 'n_count' not in adata.obs, norm_error\n",
        "        if adata.X.size < 50e6:\n",
        "            if sp.sparse.issparse(adata.X):\n",
        "                assert (adata.X.astype(int) != adata.X).nnz == 0, norm_error\n",
        "            else:\n",
        "                assert np.all(adata.X.astype(int) == adata.X), norm_error\n",
        "\n",
        "        if filter_min_counts:\n",
        "            sc.pp.filter_genes(adata, min_counts=1)\n",
        "            sc.pp.filter_cells(adata, min_counts=1)\n",
        "        if size_factors or normalize_input or logtrans_input:\n",
        "            adata.raw = adata.copy()\n",
        "        else:\n",
        "            adata.raw = adata\n",
        "        if size_factors:\n",
        "            sc.pp.normalize_total(adata)\n",
        "            adata.obs['size_factors'] = adata.obs.n_counts / \\\n",
        "                np.median(adata.obs.n_counts)\n",
        "        else:\n",
        "            adata.obs['size_factors'] = 1.0\n",
        "        if logtrans_input:\n",
        "            sc.pp.log1p(adata)\n",
        "        if highly_genes != None:\n",
        "            sc.pp.highly_variable_genes(\n",
        "                adata, min_mean=0.0125, max_mean=3, min_disp=0.7, n_top_genes=highly_genes, subset=True)\n",
        "        if normalize_input:\n",
        "            sc.pp.scale(adata)\n",
        "        return adata\n",
        "\n",
        "class Loader(object):\n",
        "    def __init__(self, config, dataset_name, drop_last=True, kwargs={}):\n",
        "        batch_size = config[\"batch_size\"]\n",
        "        self.config = config\n",
        "        train_dataset, test_dataset = self.get_dataset(dataset_name)\n",
        "        self.data_max = train_dataset.data_max\n",
        "        self.data_min = train_dataset.data_min\n",
        "\n",
        "        self.train_loader = DataLoader(\n",
        "            train_dataset, batch_size=batch_size, shuffle=True, drop_last=drop_last, **kwargs)\n",
        "        self.test_loader = DataLoader(\n",
        "            test_dataset, batch_size=batch_size*5, shuffle=False, drop_last=False, **kwargs)\n",
        "\n",
        "    def get_dataset(self, dataset_name):\n",
        "        loader_map = {'default_loader': scRNADataset}\n",
        "        dataset = loader_map[dataset_name] if dataset_name in loader_map.keys() else loader_map['default_loader']\n",
        "        train_dataset = dataset(self.config, dataset_name=dataset_name, mode='train')\n",
        "        test_dataset = dataset(self.config, dataset_name=dataset_name, mode='test')\n",
        "        return train_dataset, test_dataset\n",
        "\n",
        "def apply_noise(x, mask_probas):\n",
        "    mask = torch.bernoulli(torch.ones_like(x, device=x.device) * torch.tensor(mask_probas, device=x.device))\n",
        "    x_corrputed = x * mask\n",
        "    return x_corrputed, mask\n",
        "\n",
        "def inference(model, data_loader):\n",
        "    model.eval()\n",
        "    latent_list = []\n",
        "    label_list = []\n",
        "    with torch.no_grad():\n",
        "        for x, y in data_loader:\n",
        "            x = x.float().to(device)\n",
        "            latent = model.feature(x)\n",
        "            latent_list.append(latent.cpu().numpy())\n",
        "            label_list.append(y.numpy())\n",
        "    latent_all = np.concatenate(latent_list, axis=0)\n",
        "    label_all = np.concatenate(label_list, axis=0)\n",
        "    return latent_all, label_all\n",
        "\n",
        "def train(args):\n",
        "    data_load = Loader(args, dataset_name=args[\"dataset\"], drop_last=True)\n",
        "    data_loader = data_load.train_loader\n",
        "    data_loader_test = data_load.test_loader\n",
        "    x_shape = args[\"data_dim\"]\n",
        "\n",
        "    results = []\n",
        "    init_lr = args[\"learning_rate\"]\n",
        "    max_epochs = args[\"epochs\"]\n",
        "    mask_probas = torch.tensor([0.7]*x_shape, device=device)\n",
        "\n",
        "    model = AutoEncoder(\n",
        "        num_genes=x_shape,\n",
        "        hidden_size=128,\n",
        "        masked_data_weight=0.75,\n",
        "        mask_loss_weight=0.7\n",
        "    ).to(device)\n",
        "\n",
        "    model_checkpoint = 'model_checkpoint.pth'\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=init_lr)\n",
        "\n",
        "    for epoch in range(max_epochs):\n",
        "        model.train()\n",
        "        meter = AverageMeter()\n",
        "        for i, (x, y) in enumerate(data_loader):\n",
        "            x = x.float().to(device)\n",
        "            x_corrputed, mask = apply_noise(x, mask_probas)\n",
        "            optimizer.zero_grad()\n",
        "            x_corrputed_latent, loss_ae = model.loss_mask(x_corrputed, x, mask)\n",
        "            loss_ae.backward()\n",
        "            optimizer.step()\n",
        "            meter.update(loss_ae.detach().cpu().numpy())\n",
        "\n",
        "        if epoch % 10 == 0 or epoch == max_epochs - 1:\n",
        "            print(f\"Epoch {epoch}/{max_epochs}, Loss: {meter.avg:.4f}\")\n",
        "\n",
        "        if epoch == 80 or epoch == max_epochs - 1:\n",
        "            latent, true_label = inference(model, data_loader_test)\n",
        "\n",
        "            clustering_results = cluster_with_methods(latent, true_label, args[\"n_classes\"])\n",
        "\n",
        "            print(f\"\\nEvaluation Results at Epoch {epoch}:\")\n",
        "            print(\"=\"*60)\n",
        "            print(f\"{'Method':<15} | {'NMI':<8} | {'ARI':<8} | {'Accuracy':<8}\")\n",
        "            print(\"-\"*60)\n",
        "\n",
        "            for method, metrics in clustering_results.items():\n",
        "                res = {\n",
        "                    \"method\": method,\n",
        "                    \"nmi\": metrics['nmi'],\n",
        "                    \"ari\": metrics['ari'],\n",
        "                    \"acc\": metrics['acc'],\n",
        "                    \"dataset\": args[\"dataset\"],\n",
        "                    \"epoch\": epoch\n",
        "                }\n",
        "                results.append(res)\n",
        "                print(f\"{method:<15} | {metrics['nmi']:.4f} | {metrics['ari']:.4f} | {metrics['acc']:.4f}\")\n",
        "\n",
        "            # Visualization\n",
        "            plt.figure(figsize=(8, 6))\n",
        "            plt.scatter(latent[:, 0], latent[:, 1], c=clustering_results['Agglomerative']['labels'], cmap='tab20', s=5)\n",
        "            plt.title(f\"Agglomerative Clustering\\nNMI: {metrics['nmi']:.2f}, ARI: {metrics['ari']:.2f}\")\n",
        "            plt.colorbar()\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(os.path.join(args[\"save_path\"], f\"clustering_results_epoch_{epoch}.png\"))\n",
        "            plt.close()\n",
        "\n",
        "            np.save(os.path.join(args[\"save_path\"], f\"embedding_{epoch}.npy\"), latent)\n",
        "            pd.DataFrame({\"True\": true_label}).to_csv(os.path.join(args[\"save_path\"], f\"true_labels_{epoch}.csv\"))\n",
        "\n",
        "    torch.save({\"optimizer\": optimizer.state_dict(), \"model\": model.state_dict()}, model_checkpoint)\n",
        "\n",
        "    # Save final results\n",
        "    results_df = pd.DataFrame(results)\n",
        "    results_df.to_csv(os.path.join(args[\"save_path\"], \"final_results.csv\"), index=False)\n",
        "\n",
        "    return results\n",
        "\n",
        "#  \n",
        "if __name__ == \"__main__\":\n",
        "    # Set random seeds\n",
        "    seed = 42\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "    # Configuration\n",
        "    args = {\n",
        "        \"num_workers\": 10,\n",
        "        \"paths\": {\n",
        "            \"data\": \".\",\n",
        "            \"results\": \"./results\"\n",
        "        },\n",
        "        'batch_size': 256,\n",
        "        \"data_dim\": 1000,\n",
        "        'n_classes': 7,\n",
        "        'epochs': 150,\n",
        "        \"dataset\": \"Pollen\",\n",
        "        \"learning_rate\": 0.0008,\n",
        "        \"latent_dim\": 64,\n",
        "        \"save_path\": \"./results/Pollen\"\n",
        "    }\n",
        "\n",
        "    os.makedirs(args[\"save_path\"], exist_ok=True)\n",
        "    os.makedirs(args[\"paths\"][\"results\"], exist_ok=True)\n",
        "\n",
        "    print(\"Starting training with configuration:\")\n",
        "    print(args)\n",
        "\n",
        "    results = train(args)\n",
        "\n",
        "    print(\"\\nFinal Results Summary:\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"{'Method':<15} | {'NMI':<8} | {'ARI':<8} | {'Accuracy':<8}\")\n",
        "    print(\"-\"*60)\n",
        "\n",
        "    final_results = pd.read_csv(os.path.join(args[\"save_path\"], \"final_results.csv\"))\n",
        "    method_results = final_results[final_results['method'] == 'Agglomerative']\n",
        "    best_idx = method_results['nmi'].idxmax()\n",
        "    best = method_results.loc[best_idx]\n",
        "    print(f\"{'Agglomerative':<15} | {best['nmi']:.4f} | {best['ari']:.4f} | {best['acc']:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mPq81ZRgW6H8",
        "outputId": "048923b9-e0ae-44e6-c8b4-cccccfe3b053"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training with configuration:\n",
            "{'num_workers': 10, 'paths': {'data': '.', 'results': './results'}, 'batch_size': 256, 'data_dim': 1000, 'n_classes': 7, 'epochs': 150, 'dataset': 'Pollen', 'learning_rate': 0.0008, 'latent_dim': 64, 'save_path': './results/Pollen'}\n",
            "(301, 21721) (301, 21721) keeping 1000 genes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/legacy_api_wrap/__init__.py:82: UserWarning: If you pass `n_top_genes`, all cutoffs are ignored.\n",
            "  return fn(*args_all, **kw)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> Number of classes changed from 11 to 11 <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "(301, 21721) (301, 21721) keeping 1000 genes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/legacy_api_wrap/__init__.py:82: UserWarning: If you pass `n_top_genes`, all cutoffs are ignored.\n",
            "  return fn(*args_all, **kw)\n",
            "<ipython-input-48-727411e35ae6>:321: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  mask = torch.bernoulli(torch.ones_like(x, device=x.device) * torch.tensor(mask_probas, device=x.device))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/150, Loss: 0.7565\n",
            "Epoch 10/150, Loss: 0.6171\n",
            "Epoch 20/150, Loss: 0.5949\n",
            "Epoch 30/150, Loss: 0.5776\n",
            "Epoch 40/150, Loss: 0.5795\n",
            "Epoch 50/150, Loss: 0.5682\n",
            "Epoch 60/150, Loss: 0.5670\n",
            "Epoch 70/150, Loss: 0.5602\n",
            "Epoch 80/150, Loss: 0.5559\n",
            "\n",
            "Evaluation Results at Epoch 80:\n",
            "============================================================\n",
            "Method          | NMI      | ARI      | Accuracy\n",
            "------------------------------------------------------------\n",
            "Agglomerative   | 0.9351 | 0.9391 | 0.9269\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-48-727411e35ae6>:321: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  mask = torch.bernoulli(torch.ones_like(x, device=x.device) * torch.tensor(mask_probas, device=x.device))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 90/150, Loss: 0.5558\n",
            "Epoch 100/150, Loss: 0.5425\n",
            "Epoch 110/150, Loss: 0.5378\n",
            "Epoch 120/150, Loss: 0.5322\n",
            "Epoch 130/150, Loss: 0.5257\n",
            "Epoch 140/150, Loss: 0.5184\n",
            "Epoch 149/150, Loss: 0.5114\n",
            "\n",
            "Evaluation Results at Epoch 149:\n",
            "============================================================\n",
            "Method          | NMI      | ARI      | Accuracy\n",
            "------------------------------------------------------------\n",
            "Agglomerative   | 0.9011 | 0.8780 | 0.8904\n",
            "\n",
            "Final Results Summary:\n",
            "============================================================\n",
            "Method          | NMI      | ARI      | Accuracy\n",
            "------------------------------------------------------------\n",
            "Agglomerative   | 0.9351 | 0.9391 | 0.9269\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}